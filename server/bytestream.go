package server

import (
	"context"
	"errors"
	"fmt"
	"io"
	"strconv"
	"strings"
	"time"

	"github.com/aws/aws-sdk-go/service/s3"
	execpb "github.com/bazelbuild/remote-apis/build/bazel/remote/execution/v2"
	"github.com/golang/protobuf/proto"
	"github.com/ricochet1k/buildbuildbuild/utils/s3utils"
	"github.com/sirupsen/logrus"
	bytestreampb "google.golang.org/genproto/googleapis/bytestream"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
)

type FakeWriterAt struct {
	w io.Writer
}

func (fw FakeWriterAt) WriteAt(p []byte, offset int64) (n int, err error) {
	return fw.w.Write(p)
}

// CAS:
// For small file uploads the client should group them together and call
// [BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs]
// on chunks of no more than 10 MiB. For large uploads, the client must use the
// [Write method][google.bytestream.ByteStream.Write] of the ByteStream API. The
// `resource_name` is `{instance_name}/uploads/{uuid}/blobs/{hash}/{size}`,
// where `instance_name` is as described in the next paragraph, `uuid` is a
// version 4 UUID generated by the client, and `hash` and `size` are the
// [Digest][build.bazel.remote.execution.v2.Digest] of the blob. The
// `uuid` is used only to avoid collisions when multiple clients try to upload
// the same file (or the same client tries to upload the file multiple times at
// once on different threads), so the client MAY reuse the `uuid` for uploading
// different blobs. The `resource_name` may optionally have a trailing filename
// (or other metadata) for a client to use if it is storing URLs, as in
// `{instance}/uploads/{uuid}/blobs/{hash}/{size}/foo/bar/baz.cc`. Anything
// after the `size` is ignored.

type casResourceName struct {
	instanceName string
	uuid         string
	digest       *execpb.Digest
	filename     string
}

func parseResourceName(resourceName string) (*casResourceName, error) {
	parts := strings.SplitN(resourceName, "/", 7)
	if (len(parts) == 6 || len(parts) == 7) && parts[1] == "uploads" && parts[3] == "blobs" {
		size, err := strconv.ParseInt(parts[5], 10, 64)
		if err != nil {
			return nil, err
		}
		filename := ""
		if len(parts) == 7 {
			filename = parts[6]
		}
		return &casResourceName{
			instanceName: parts[0],
			uuid:         parts[2],
			digest: &execpb.Digest{
				Hash:      parts[4],
				SizeBytes: size,
			},
			filename: filename,
		}, nil
	}

	// Read shows up like `{instance_name}/blobs/{hash}/{size}`
	if len(parts) == 4 && parts[1] == "blobs" {
		size, err := strconv.ParseInt(parts[3], 10, 64)
		if err != nil {
			return nil, err
		}
		return &casResourceName{
			instanceName: parts[0],
			uuid:         "",
			digest: &execpb.Digest{
				Hash:      parts[2],
				SizeBytes: size,
			},
			filename: "",
		}, nil
	}

	logrus.Infof("BS unrecognized resource_name format: %v\n", resourceName)

	return nil, fmt.Errorf("unrecognized resource_name format")
}

// `Read()` is used to retrieve the contents of a resource as a sequence
// of bytes. The bytes are returned in a sequence of responses, and the
// responses are delivered as the results of a server-side streaming RPC.
func (c *Server) Read(req *bytestreampb.ReadRequest, rs bytestreampb.ByteStream_ReadServer) error {
	resname, err := parseResourceName(req.ResourceName)
	if err != nil {
		return err
	}
	key := StorageKey(resname.instanceName, CONTENT_CAS, DigestKey(resname.digest))

	// logrus.Infof("ByteStream Downloading %q\n", req.ResourceName)

	re, wr := io.Pipe()
	senderr := make(chan error)
	go func() {
		defer close(senderr)
		var buf [25 * 1024]byte
		for {
			n, err := re.Read(buf[:])
			if err != nil { // EOF or not, we need to close and return
				re.CloseWithError(err)
				senderr <- err
				return
			}
			err = rs.Send(&bytestreampb.ReadResponse{
				Data: buf[:n],
			})
			if err != nil {
				re.CloseWithError(err)
				senderr <- err
				return
			}
		}
	}()

	var dlrange *string
	if req.ReadOffset > 0 || req.ReadLimit > 0 {
		dlrange = proto.String(fmt.Sprintf("bytes=%v-%v", req.ReadOffset, req.ReadOffset+req.ReadLimit))
	}

	_, err = c.downloaderNoConcurrency.DownloadWithContext(rs.Context(), FakeWriterAt{wr}, &s3.GetObjectInput{
		Bucket: &c.bucket,
		Key:    &key,
		Range:  dlrange,
	})
	wr.CloseWithError(err)
	if err != nil {
		return err
	}

	// logrus.Infof("ByteStream Download done %q\n", req.ResourceName)

	return <-senderr
}

// `Write()` is used to send the contents of a resource as a sequence of
// bytes. The bytes are sent in a sequence of request protos of a client-side
// streaming RPC.
//
// A `Write()` action is resumable. If there is an error or the connection is
// broken during the `Write()`, the client should check the status of the
// `Write()` by calling `QueryWriteStatus()` and continue writing from the
// returned `committed_size`. This may be less than the amount of data the
// client previously sent.
//
// Calling `Write()` on a resource name that was previously written and
// finalized could cause an error, depending on whether the underlying service
// allows over-writing of previously written resources.
//
// When the client closes the request channel, the service will respond with
// a `WriteResponse`. The service will not view the resource as `complete`
// until the client has sent a `WriteRequest` with `finish_write` set to
// `true`. Sending any requests on a stream after sending a request with
// `finish_write` set to `true` will cause an error. The client **should**
// check the `WriteResponse` it receives to determine how much data the
// service was able to commit and whether the service views the resource as
// `complete` or not.
func (c *Server) Write(ws bytestreampb.ByteStream_WriteServer) error {
	req, err := ws.Recv()
	if err != nil {
		return err
	}

	resourceName := req.ResourceName
	resname, err := parseResourceName(resourceName)
	if err != nil {
		return err
	}
	key := StorageKey(resname.instanceName, CONTENT_CAS, DigestKey(resname.digest))

	w, err := s3utils.NewS3MultipartUploader(ws.Context(), c.uploader.S3, &c.bufPool, c.bucket, key, int(resname.digest.SizeBytes))
	if err != nil {
		return err
	}

	// logrus.Infof("ByteStream Uploading %q -> %q\n", resourceName, key)

	c.uploads.Store(resourceName, uploadStatus{
		committedBytes: 0,
		complete:       false,
	})
	defer func() {
		go func() {
			time.Sleep(10 * time.Minute)
			c.uploads.Delete(resourceName)
		}()
	}()

	if req.WriteOffset > 0 {
		if _, err := w.Seek(0, int(req.WriteOffset)); err != nil {
			return err
		}
	}

	for {
		_, _ = w.Write(req.Data)

		if req.FinishWrite {
			break
		}

		req, err = ws.Recv()
		if err != nil {
			if errors.Is(err, io.EOF) {
				break
			}
			logrus.Infof("   BS Upload Recv Err: %v %v\n", resname.digest.SizeBytes, err)
			w.Close()
			return err
		}
	}

	if err = w.Close(); err != nil {
		logrus.Infof("   BS Upload Wait Err: %v %v\n", resname.digest.SizeBytes, err)
		return err
	}

	c.uploads.Store(resourceName, uploadStatus{
		committedBytes: resname.digest.SizeBytes,
		complete:       true,
	})

	// logrus.Infof("   BS SendAndClose %q %v\n", resourceName, totalBytes)
	return ws.SendAndClose(&bytestreampb.WriteResponse{
		CommittedSize: resname.digest.SizeBytes,
	})
}

// `QueryWriteStatus()` is used to find the `committed_size` for a resource
// that is being written, which can then be used as the `write_offset` for
// the next `Write()` call.
//
// If the resource does not exist (i.e., the resource has been deleted, or the
// first `Write()` has not yet reached the service), this method returns the
// error `NOT_FOUND`.
//
// The client **may** call `QueryWriteStatus()` at any time to determine how
// much data has been processed for this resource. This is useful if the
// client is buffering data and needs to know which data can be safely
// evicted. For any sequence of `QueryWriteStatus()` calls for a given
// resource name, the sequence of returned `committed_size` values will be
// non-decreasing.
func (c *Server) QueryWriteStatus(ctx context.Context, req *bytestreampb.QueryWriteStatusRequest) (*bytestreampb.QueryWriteStatusResponse, error) {
	value, ok := c.uploads.Load(req.ResourceName)
	if !ok {
		return nil, status.Error(codes.NotFound, "not found")
	}

	stat := value.(uploadStatus)

	// logrus.Infof("  QueryWriteStatus: %v %v\n", req.ResourceName, stat)

	return &bytestreampb.QueryWriteStatusResponse{
		CommittedSize: stat.committedBytes,
		Complete:      stat.complete,
	}, nil
}
