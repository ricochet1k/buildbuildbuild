package server

import (
	"context"
	"errors"
	"fmt"
	"io"
	"strconv"
	"strings"
	"sync"
	"time"

	execpb "github.com/bazelbuild/remote-apis/build/bazel/remote/execution/v2"
	"github.com/ricochet1k/buildbuildbuild/storage"
	"github.com/ricochet1k/buildbuildbuild/utils"
	"github.com/sirupsen/logrus"
	bytestreampb "google.golang.org/genproto/googleapis/bytestream"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
)

// CAS:
//
// For uncompressed data, The `WriteRequest.resource_name` is of the following form:
// `{instance_name}/uploads/{uuid}/blobs/{hash}/{size}{/optional_metadata}`
//
// Data can alternatively be uploaded in compressed form, with the following
// `WriteRequest.resource_name` form:
// `{instance_name}/uploads/{uuid}/compressed-blobs/{compressor}/{uncompressed_hash}/{uncompressed_size}{/optional_metadata}`
//
// For uncompressed data, The `ReadRequest.resource_name` is of the following form:
// `{instance_name}/blobs/{hash}/{size}`
// Where `instance_name`, `hash` and `size` are defined as for uploads.
//
// Data can alternatively be downloaded in compressed form, with the following
// `ReadRequest.resource_name` form:
// `{instance_name}/compressed-blobs/{compressor}/{uncompressed_hash}/{uncompressed_size}`
//
// Where:
// * `instance_name` is an identifier, possibly containing multiple path
//   segments, used to distinguish between the various instances on the server,
//   in a manner defined by the server. If it is the empty path, the leading
//   slash is omitted, so that  the `resource_name` becomes
//   `uploads/{uuid}/blobs/{hash}/{size}{/optional_metadata}`.
//   To simplify parsing, a path segment cannot equal any of the following
//   keywords: `blobs`, `uploads`, `actions`, `actionResults`, `operations`,
//   `capabilities` or `compressed-blobs`.
// * `uuid` is a version 4 UUID generated by the client, used to avoid
//   collisions between concurrent uploads of the same data. Clients MAY
//   reuse the same `uuid` for uploading different blobs.
// * `hash` and `size` refer to the [Digest][build.bazel.remote.execution.v2.Digest]
//   of the data being uploaded.
// * `optional_metadata` is implementation specific data, which clients MAY omit.
//   Servers MAY ignore this metadata.
//
// For small file uploads the client should group them together and call
// [BatchUpdateBlobs][build.bazel.remote.execution.v2.ContentAddressableStorage.BatchUpdateBlobs]
// on chunks of no more than 10 MiB. For large uploads, the client must use the
// [Write method][google.bytestream.ByteStream.Write] of the ByteStream API. The
// `resource_name` is `{instance_name}/uploads/{uuid}/blobs/{hash}/{size}`,
// where `instance_name` is as described in the next paragraph, `uuid` is a
// version 4 UUID generated by the client, and `hash` and `size` are the
// [Digest][build.bazel.remote.execution.v2.Digest] of the blob. The
// `uuid` is used only to avoid collisions when multiple clients try to upload
// the same file (or the same client tries to upload the file multiple times at
// once on different threads), so the client MAY reuse the `uuid` for uploading
// different blobs. The `resource_name` may optionally have a trailing filename
// (or other metadata) for a client to use if it is storing URLs, as in
// `{instance}/uploads/{uuid}/blobs/{hash}/{size}/foo/bar/baz.cc`. Anything
// after the `size` is ignored.

var reservedPathSegments = map[string]bool{
	"blobs":            true,
	"uploads":          true,
	"actions":          true,
	"actionResults":    true,
	"operations":       true,
	"capabilities":     true,
	"compressed-blobs": true,
}

type casResourceName struct {
	instanceName   string
	uuid           string
	compressor     string
	localcacheonly bool
	digest         execpb.Digest
	filename       string
}

func (resname *casResourceName) toBlobKey() (storage.BlobKey, error) {
	key := storage.BlobKey{
		InstanceName: resname.instanceName,
		Kind:         storage.CONTENT_CAS,
		Digest:       resname.digest.Hash,
		Size:         int(resname.digest.SizeBytes),
	}

	if resname.compressor == "" {

	} else if resname.compressor == "zstd" {
		key.Compressor = execpb.Compressor_ZSTD
	} else {
		return key, status.Error(codes.InvalidArgument, "unsupported compressor")
	}

	if resname.localcacheonly {
		kind, k, _ := strings.Cut(resname.filename, "/")
		key.Kind = storage.ContentKind(kind)
		key.Key = k
		key.Digest = ""
		key.Size = -1
	}

	return key, nil
}

func parseResourceName(resourceName string) (*casResourceName, error) {
	resname := &casResourceName{}

	parts := strings.Split(resourceName, "/")
	partsLen := len(parts)

	i := 0

	// first figure out the instance name
	for i < partsLen-1 && !reservedPathSegments[parts[i]] {
		i += 1
	}
	resname.instanceName = strings.Join(parts[:i], "/")

	if partsLen-i < 2 {
		return nil, fmt.Errorf("resourceName not long enough: %q  %q", resourceName, parts)
	}

	if parts[i] == "uploads" {
		resname.uuid = parts[i+1]
		i += 2
	}

	switch parts[i] {
	case "compressed-blobs":
		resname.compressor = parts[i+1]
		i += 2
	case "blobs":
		i += 1
	default:
		logrus.Errorf("BS unrecognized resource_name format: %v\n", resourceName)
		return nil, fmt.Errorf("unrecognized resource_name format: %q", resourceName)
	}

	// non-standard extension: cache-only read, used by storage.CacheStorage
	// since this is an invalid digest, there are no compatibility concerns
	if parts[i] == "localcacheonly" {
		resname.localcacheonly = true
		i += 1
		resname.filename = strings.Join(parts[i:], "/")
		return resname, nil
	}

	if partsLen-i < 2 {
		return nil, fmt.Errorf("resourceName not long enough: %q  %q", resourceName, parts)
	}

	resname.digest.Hash = parts[i]
	i += 1

	size, err := strconv.ParseInt(parts[i], 10, 64)
	if err != nil {
		return nil, err
	}
	resname.digest.SizeBytes = size
	i += 1

	resname.filename = strings.Join(parts[i:], "/")

	return resname, err
}

// `Read()` is used to retrieve the contents of a resource as a sequence
// of bytes. The bytes are returned in a sequence of responses, and the
// responses are delivered as the results of a server-side streaming RPC.
func (c *Server) Read(req *bytestreampb.ReadRequest, rs bytestreampb.ByteStream_ReadServer) error {
	resourceName := req.ResourceName
	resname, err := parseResourceName(resourceName)
	if err != nil {
		logrus.Errorf("ByteStream.Read invalid resourceName %q: %v", resourceName, err)
		return err
	}

	key, err := resname.toBlobKey()
	if err != nil {
		return err
	}

	key.SetMetadataFromContext(rs.Context())

	if utils.DebugFilter(resourceName) {
		logrus.Debugf("ByteStream Read %q (%v, %v)\n", req.ResourceName, req.ReadOffset, req.ReadLimit)
	}
	store := c.Storage
	if resname.localcacheonly {
		store = c.LocalStorage
	}

	r, _, err := store.DownloadReader(rs.Context(), key, int(req.ReadOffset), int(req.ReadLimit))
	if err != nil {
		if utils.DebugFilter(resourceName) {
			logrus.Debugf("ByteStream Read ERROR %v: %v\n", key, err)
		}
		return err
	}

	w := utils.NewBSReadWriter(rs)

	if utils.DebugFilter(resourceName) {
		logrus.Debugf("ReadImpl: %#v %#v", w, r)
	}

	_, err = io.Copy(w, r)

	if err != nil {
		if utils.DebugFilter(resourceName) {
			logrus.Debugf("ByteStream Read err %q - %v\n", req.ResourceName, err)
		}
	}

	return err
}

type pausedWrite struct {
	resourceName string
	w            utils.WriteCloserFinish
}

type uploadStatus struct {
	mutex          sync.Mutex
	committedBytes int64
	complete       bool
	paused         *pausedWrite
}

// `Write()` is used to send the contents of a resource as a sequence of
// bytes. The bytes are sent in a sequence of request protos of a client-side
// streaming RPC.
//
// A `Write()` action is resumable. If there is an error or the connection is
// broken during the `Write()`, the client should check the status of the
// `Write()` by calling `QueryWriteStatus()` and continue writing from the
// returned `committed_size`. This may be less than the amount of data the
// client previously sent.
//
// Calling `Write()` on a resource name that was previously written and
// finalized could cause an error, depending on whether the underlying service
// allows over-writing of previously written resources.
//
// When the client closes the request channel, the service will respond with
// a `WriteResponse`. The service will not view the resource as `complete`
// until the client has sent a `WriteRequest` with `finish_write` set to
// `true`. Sending any requests on a stream after sending a request with
// `finish_write` set to `true` will cause an error. The client **should**
// check the `WriteResponse` it receives to determine how much data the
// service was able to commit and whether the service views the resource as
// `complete` or not.
func (c *Server) Write(ws bytestreampb.ByteStream_WriteServer) (finalerr error) {
	// TODO: verify hash
	req, err := ws.Recv()
	if err != nil {
		return err
	}

	resourceName := req.ResourceName
	resname, err := parseResourceName(resourceName)
	if err != nil {
		logrus.Errorf("ByteStream.Write invalid resourceName %q: %v", resourceName, err)
		return err
	}

	key, err := resname.toBlobKey()
	if err != nil {
		return err
	}
	key.SetMetadataFromContext(ws.Context())

	var upload *uploadStatus
	var w utils.WriteCloserFinish
	if req.WriteOffset > 0 {
		// try to reconnect to an already-open write stream
		val, ok := c.uploads.Load(resourceName)
		if !ok {
			return status.Error(codes.OutOfRange, "Cannot resume upload: missing")
		}
		upload = val.(*uploadStatus)
		upload.mutex.Lock()
		if upload.paused == nil || upload.paused.w == nil {
			upload.mutex.Unlock()
			return status.Error(codes.OutOfRange, "Cannot resume upload: not live")
		}

		if req.WriteOffset != upload.committedBytes {
			upload.mutex.Unlock()
			return status.Error(codes.OutOfRange, fmt.Sprintf("Cannot resume upload: wrong WriteOffset: %v != %v", req.WriteOffset, upload.committedBytes))
		}

		logrus.Debugf("Resuming upload: %q %v", resourceName, req.WriteOffset)

		pw := upload.paused
		upload.paused = nil
		upload.mutex.Unlock()
		w = pw.w
	} else {
		store := c.Storage
		if resname.localcacheonly {
			store = c.LocalStorage
		}

		w, err = store.UploadWriter(ws.Context(), key)
		if err != nil {
			return err
		}

		upload = &uploadStatus{}
		c.uploads.Store(resourceName, upload)
	}

	if w == nil {
		logrus.Panicf("nil writer! %q", req)
	}

	if utils.DebugFilter(resourceName) {
		logrus.Debugf("ByteStream Write %q: %#v\n", resourceName, w)
	}
	defer func() {
		// if finalerr != nil {
		if utils.DebugFilter(resourceName) {
			logrus.Infof("   BS Error %q: %v\n", resourceName, finalerr)
		}
		// }
	}()

	defer func() {
		go func() {
			time.Sleep(10 * time.Minute)
			c.uploads.Delete(resourceName)
		}()
	}()

	for {
		_, err = w.Write(req.Data)
		if err != nil {
			w.Close()
			return err
		}

		upload.mutex.Lock()
		upload.committedBytes += int64(len(req.Data))
		// logrus.Infof("BS Write chunk: %q : at(%v) %v %v. %v", resourceName, req.WriteOffset, dataLen, len(req.Data), upload.committedBytes)
		upload.complete = req.FinishWrite
		upload.mutex.Unlock()

		if req.FinishWrite {
			break
		}

		req, err = ws.Recv()
		if err != nil {
			if errors.Is(err, io.EOF) {
				break
			}
			// logrus.Infof("   BS Upload Recv Err: %v %v\n", resname.digest.SizeBytes, err)

			// Rather than closing the writer, upload is supposed to be able to resume by calling QueryWriteStatus

			break
		}
	}

	if req != nil && req.FinishWrite {
		if err = w.Finish(); err != nil {
			if utils.DebugFilter(resourceName) {
				logrus.Errorf("BS Upload Finish Err: %v %v %v %v", resourceName, resname, key, err)
			}
			return err
		}
		if err = w.Close(); err != nil {
			if utils.DebugFilter(resourceName) {
				logrus.Errorf("BS Upload Close Err: %v %v %v %v", resourceName, resname, key, err)
			}
			return err
		}

		if utils.DebugFilter(resourceName) {
			logrus.Debugf("   BS SendAndClose %q\n", resourceName)
		}
		return ws.SendAndClose(&bytestreampb.WriteResponse{CommittedSize: upload.committedBytes})
	} else {
		upload.mutex.Lock()
		upload.paused = &pausedWrite{
			resourceName: resourceName,
			w:            w,
		}
		upload.mutex.Unlock()

		return err
	}
}

// `QueryWriteStatus()` is used to find the `committed_size` for a resource
// that is being written, which can then be used as the `write_offset` for
// the next `Write()` call.
//
// If the resource does not exist (i.e., the resource has been deleted, or the
// first `Write()` has not yet reached the service), this method returns the
// error `NOT_FOUND`.
//
// The client **may** call `QueryWriteStatus()` at any time to determine how
// much data has been processed for this resource. This is useful if the
// client is buffering data and needs to know which data can be safely
// evicted. For any sequence of `QueryWriteStatus()` calls for a given
// resource name, the sequence of returned `committed_size` values will be
// non-decreasing.
func (c *Server) QueryWriteStatus(ctx context.Context, req *bytestreampb.QueryWriteStatusRequest) (*bytestreampb.QueryWriteStatusResponse, error) {
	value, ok := c.uploads.Load(req.ResourceName)
	if !ok {
		return nil, status.Error(codes.NotFound, "not found")
	}

	upload := value.(*uploadStatus)
	upload.mutex.Lock()
	defer upload.mutex.Lock()

	// logrus.Infof("  QueryWriteStatus: %v %v\n", req.ResourceName, upload)

	return &bytestreampb.QueryWriteStatusResponse{
		CommittedSize: upload.committedBytes,
		Complete:      upload.complete,
	}, nil
}
